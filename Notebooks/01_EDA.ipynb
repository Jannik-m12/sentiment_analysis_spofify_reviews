{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b8332e",
   "metadata": {},
   "source": [
    "# EDA (statistics, quality checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5116b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ“Š Text Statistics\n",
      "==================================================\n",
      "       text_len  word_count\n",
      "count    100.00      100.00\n",
      "mean     117.02       21.30\n",
      "std      108.00       20.11\n",
      "min       24.00        3.00\n",
      "25%       43.00        8.00\n",
      "50%       78.50       13.50\n",
      "75%      134.50       24.25\n",
      "max      500.00      106.00\n",
      "\n",
      "==================================================\n",
      "âš ï¸  Noise / Non-ASCII ratio\n",
      "==================================================\n",
      "  Rows with emojis / non-ASCII chars: 17 (17.00 %)\n",
      "\n",
      "==================================================\n",
      "ğŸ”¤ Top 10 Most Common Words\n",
      "==================================================\n",
      "   word  count\n",
      "     to     74\n",
      "    the     70\n",
      "    app     63\n",
      "premium     48\n",
      "   this     46\n",
      "     is     43\n",
      "    and     35\n",
      "      i     33\n",
      "spotify     30\n",
      "    now     29\n",
      "\n",
      "==================================================\n",
      "ğŸ” Quirks / Data Quality Flags\n",
      "==================================================\n",
      "  âœ…  Missing / NaN texts                : 0\n",
      "  âœ…  Blank texts                        : 0\n",
      "  âœ…  Duplicate texts                    : 0\n",
      "  âœ…  Very short texts (< 3 w)           : 0\n",
      "  ğŸš©  Very long texts (> 99 pct)         : 1\n",
      "  ğŸš©  Non-ASCII / emoji texts            : 17\n",
      "\n",
      "==================================================\n",
      "ğŸ“ 3 Random Raw Samples\n",
      "==================================================\n",
      "\n",
      "  [1] u just make everything paid trash app\n",
      "\n",
      "  [2] I really love this app this app always give me best jazz to Jinja my self\n",
      "\n",
      "  [3] Where is hip hop Pretty rude No baby Little baby\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# â”€â”€ Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv('../Data/spotify_reviews.csv')\n",
    "\n",
    "\n",
    "# â”€â”€ Basic text features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['text_len']      = df['Content'].str.len()\n",
    "df['word_count']    = df['Content'].str.split().str.len()\n",
    "df['has_non_ascii'] = df['Content'].str.contains(r'[^\\x00-\\x7F]', regex=True)\n",
    "\n",
    "# â”€â”€ Descriptive stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š Text Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(df[['text_len', 'word_count']].describe().round(2))\n",
    "\n",
    "# â”€â”€ Noise ratio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âš ï¸  Noise / Non-ASCII ratio\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Rows with emojis / non-ASCII chars: \"\n",
    "    f\"{df['has_non_ascii'].sum()} \"\n",
    "    f\"({df['has_non_ascii'].mean()*100:.2f} %)\")\n",
    "\n",
    "# â”€â”€ Top 10 words â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”¤ Top 10 Most Common Words\")\n",
    "print(\"=\" * 50)\n",
    "top_words = (\n",
    "    df['Content']\n",
    "    .str.lower()\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)   # strip punctuation first\n",
    "    .str.split()\n",
    "    .explode()\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .reset_index()\n",
    ")\n",
    "top_words.columns = ['word', 'count']\n",
    "print(top_words.to_string(index=False))\n",
    "\n",
    "# â”€â”€ Quirks list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ” Quirks / Data Quality Flags\")\n",
    "print(\"=\" * 50)\n",
    "empty_rows      = df['Content'].isna().sum()\n",
    "blank_rows      = (df['Content'].str.strip() == '').sum()\n",
    "duplicate_rows  = df['Content'].duplicated().sum()\n",
    "very_short      = (df['word_count'] < 3).sum()\n",
    "very_long       = (df['word_count'] > df['word_count'].quantile(0.99)).sum()\n",
    "\n",
    "quirks = {\n",
    "    \"Missing / NaN texts\"       : empty_rows,\n",
    "    \"Blank texts\"               : blank_rows,\n",
    "    \"Duplicate texts\"           : duplicate_rows,\n",
    "    \"Very short texts (< 3 w)\"  : very_short,\n",
    "    \"Very long texts (> 99 pct)\": very_long,\n",
    "    \"Non-ASCII / emoji texts\"   : df['has_non_ascii'].sum(),\n",
    "}\n",
    "for label, val in quirks.items():\n",
    "    flag = \"ğŸš©\" if val > 0 else \"âœ…\"\n",
    "    print(f\"  {flag}  {label:<35}: {val}\")\n",
    "\n",
    "# â”€â”€ 3 raw samples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“ 3 Random Raw Samples\")\n",
    "print(\"=\" * 50)\n",
    "for i, sample in enumerate(df['Content'].sample(3, random_state=42).tolist(), 1):\n",
    "    print(f\"\\n  [{i}] {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21599f82",
   "metadata": {},
   "source": [
    "# Punctuation feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa73f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Extract punctuation intensity BEFORE cleaning\n",
    "df['exclamation_count'] = df['Content'].str.count(r'!')\n",
    "df['question_count'] = df['Content'].str.count(r'\\?')\n",
    "df['has_repeated_punct'] = df['Content'].str.contains(r'[!?]{2,}', regex=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6028",
   "metadata": {},
   "source": [
    "# Modular preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b999a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” Preprocessing Pipeline Preview (3 samples)\n",
      "============================================================\n",
      "\n",
      "  [1] Raw     : Spotify is very good, got many songs and suggests new ones but I've to say the a...\n",
      "      Cleaned : spotify is very good, got many songs and suggests new ones but i have to say the...\n",
      "\n",
      "  [2] Raw     : Worst update ever!!! Now, you can't even forward or backward lyrics/part in a so...\n",
      "      Cleaned : worst update ever!!! now, you cannot even forward or backward lyrics/part in a s...\n",
      "\n",
      "  [3] Raw     : Intolerable it is, as they are now demanding remuneration for the recurring audi...\n",
      "      Cleaned : intolerable it is, as they are now demanding remuneration for the recurring audi...\n",
      "\n",
      "ğŸ“Š Vocabulary size: 719 unique words\n",
      "\n",
      "==================================================\n",
      "ğŸ“ Sequence Length Stats (post-cleaning)\n",
      "==================================================\n",
      "count    100.00\n",
      "mean      21.79\n",
      "std       20.59\n",
      "min        3.00\n",
      "25%        8.75\n",
      "50%       14.00\n",
      "75%       24.25\n",
      "max      110.00\n",
      "Name: token_count, dtype: float64\n",
      "\n",
      "  Avg tokens/review : 21.8\n",
      "  Median            : 14.0\n",
      "  95th percentile   : 66\n",
      "\n",
      "  â†’ Relevant for max_length in transformer models (e.g. BERT = 512 tokens)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "# â”€â”€ Modular preprocessing function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline.\n",
    "    Handles: HTML, Unicode, contractions, URLs, social handles, punctuation.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 2. Fix contractions & lowercase (SKIP ASCII normalization)\n",
    "    text = contractions.fix(text).lower()\n",
    "    \n",
    "    # 3. Remove URLs, handles, hashtags\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # 4. Keep punctuation and emojis\n",
    "    \n",
    "    # 5. Normalize whitespace\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# â”€â”€ Apply preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['cleaned'] = df['Content'].apply(preprocess_text)\n",
    "\n",
    "# â”€â”€ Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” Preprocessing Pipeline Preview (3 samples)\")\n",
    "print(\"=\" * 60)\n",
    "for i, (raw, cleaned) in enumerate(zip(df['Content'].head(3), df['cleaned'].head(3)), 1):\n",
    "    print(f\"\\n  [{i}] Raw     : {raw[:80]}...\")\n",
    "    print(f\"      Cleaned : {cleaned[:80]}...\")\n",
    "\n",
    "\n",
    "# â”€â”€ Vocabulary check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vocab_size = len(set(' '.join(df['cleaned']).split()))\n",
    "print(f\"\\nğŸ“Š Vocabulary size: {vocab_size:,} unique words\")\n",
    "\n",
    "# â”€â”€ Sequence Length Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['token_count'] = df['cleaned'].str.split().str.len()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“ Sequence Length Stats (post-cleaning)\")\n",
    "print(\"=\" * 50)\n",
    "print(df['token_count'].describe().round(2))\n",
    "print(f\"\\n  Avg tokens/review : {df['token_count'].mean():.1f}\")\n",
    "print(f\"  Median            : {df['token_count'].median():.1f}\")\n",
    "print(f\"  95th percentile   : {df['token_count'].quantile(0.95):.0f}\")\n",
    "print(f\"\\n  â†’ Relevant for max_length in transformer models (e.g. BERT = 512 tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9829ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” Manual Inspection: Raw vs Cleaned (10 Random Samples)\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "  Raw     : u just make everything paid trash app...\n",
      "  Cleaned : you just make everything paid trash app...\n",
      "  Length  : 37 â†’ 39 chars\n",
      "\n",
      "[Sample 2]\n",
      "  Raw     : I really love this app this app always give me best jazz to Jinja my self...\n",
      "  Cleaned : i really love this app this app always give me best jazz to jinja my self...\n",
      "  Length  : 73 â†’ 73 chars\n",
      "\n",
      "[Sample 3]\n",
      "  Raw     : Where is hip hop Pretty rude No baby Little baby...\n",
      "  Cleaned : where is hip hop pretty rude no baby little baby...\n",
      "  Length  : 48 â†’ 48 chars\n",
      "\n",
      "[Sample 4]\n",
      "  Raw     : wrost app ever , every time show there subscribtion plan and didn't play music we'll...\n",
      "  Cleaned : wrost app ever , every time show there subscribtion plan and did not play music we will...\n",
      "  Length  : 84 â†’ 87 chars\n",
      "\n",
      "[Sample 5]\n",
      "  Raw     : I don't know why people's are gave Less *. But I thought its Like gems of the Music....\n",
      "  Cleaned : i do not know why people's are gave less *. but i thought its like gems of the music....\n",
      "  Length  : 84 â†’ 85 chars\n",
      "\n",
      "[Sample 6]\n",
      "  Raw     : The worst app ever...I can't even listen to songs in the order I want to...like really?..b...\n",
      "  Cleaned : the worst app ever...i cannot even listen to songs in the order i want to...like really?.....\n",
      "  Length  : 98 â†’ 99 chars\n",
      "\n",
      "[Sample 7]\n",
      "  Raw     : Spotify think he is only one music app for Indians that's why he change his T&C for free u...\n",
      "  Cleaned : spotify think he is only one music app for indians that is why he change his t&c for free ...\n",
      "  Length  : 148 â†’ 149 chars\n",
      "\n",
      "[Sample 8]\n",
      "  Raw     : Removed basic features what the hackğŸ˜‚...\n",
      "  Cleaned : removed basic features what the hackğŸ˜‚...\n",
      "  Length  : 37 â†’ 37 chars\n",
      "\n",
      "[Sample 9]\n",
      "  Raw     : this app Was perfect before and now because of this update tht removed basic features has ...\n",
      "  Cleaned : this app was perfect before and now because of this update tht removed basic features has ...\n",
      "  Length  : 259 â†’ 266 chars\n",
      "\n",
      "[Sample 10]\n",
      "  Raw     : Spotify is very good, got many songs and suggests new ones but I've to say the app was way...\n",
      "  Cleaned : spotify is very good, got many songs and suggests new ones but i have to say the app was w...\n",
      "  Length  : 500 â†’ 505 chars\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ Cleanup Assessment for Sentiment Analysis\n",
      "================================================================================\n",
      "\n",
      "Reflection Questions:\n",
      "  âœ“ Are sentiment-bearing words preserved? (e.g., \"hate\", \"love\", \"worst\")\n",
      "  âœ“ Are punctuation marks kept? (e.g., \"!!!\", \"??\" â†’ important for intensity)\n",
      "  âœ“ Did we lose important negations? (e.g., \"can't\", \"won't\" â†’ contractions fixed?)\n",
      "  âœ“ Are there URL remnants, handles, or HTML tags left? (should be gone)\n",
      "  âœ“ Is the text readable and understandable?\n",
      "\n",
      "\"For SENTIMENT ANALYSIS: Emojis and punctuation are PRESERVED as sentiment signals.\"\n",
      "If you see issues â†’ adjust preprocess_text() function regex patterns.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Signal vs Noise Summary\n",
      "================================================================================\n",
      "  Total characters removed: -94 (-0.8%)\n",
      "  Average text reduction  : -0.9 chars/review\n",
      "\n",
      "  â†’ If >50% removed: Check if over-cleaning. Numbers/symbols may be important.\n",
      "  â†’ If <10% removed: Check if under-cleaning. Noise may still be present.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Manual Inspection: Sample 10 Cleaned Records â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” Manual Inspection: Raw vs Cleaned (10 Random Samples)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sample 10 random records\n",
    "sample_indices = df.sample(10, random_state=42).index\n",
    "\n",
    "for idx, i in enumerate(sample_indices, 1):\n",
    "    raw = df.loc[i, 'Content']\n",
    "    cleaned = df.loc[i, 'cleaned']\n",
    "    \n",
    "    print(f\"\\n[Sample {idx}]\")\n",
    "    print(f\"  Raw     : {raw[:90]}...\")\n",
    "    print(f\"  Cleaned : {cleaned[:90]}...\")\n",
    "    print(f\"  Length  : {len(raw)} â†’ {len(cleaned)} chars\")\n",
    "\n",
    "# â”€â”€ Cleanup Assessment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“‹ Cleanup Assessment for Sentiment Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Reflection Questions:\n",
    "  âœ“ Are sentiment-bearing words preserved? (e.g., \"hate\", \"love\", \"worst\")\n",
    "  âœ“ Are punctuation marks kept? (e.g., \"!!!\", \"??\" â†’ important for intensity)\n",
    "  âœ“ Did we lose important negations? (e.g., \"can't\", \"won't\" â†’ contractions fixed?)\n",
    "  âœ“ Are there URL remnants, handles, or HTML tags left? (should be gone)\n",
    "  âœ“ Is the text readable and understandable?\n",
    "\n",
    "\"For SENTIMENT ANALYSIS: Emojis and punctuation are PRESERVED as sentiment signals.\"\n",
    "If you see issues â†’ adjust preprocess_text() function regex patterns.\n",
    "\"\"\")\n",
    "\n",
    "# â”€â”€ Signal vs Noise Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š Signal vs Noise Summary\")\n",
    "print(\"=\" * 80)\n",
    "chars_removed = (df['Content'].str.len() - df['cleaned'].str.len()).sum()\n",
    "pct_removed = (chars_removed / df['Content'].str.len().sum()) * 100\n",
    "print(f\"  Total characters removed: {chars_removed:,} ({pct_removed:.1f}%)\")\n",
    "print(f\"  Average text reduction  : {(df['Content'].str.len() - df['cleaned'].str.len()).mean():.1f} chars/review\")\n",
    "print(f\"\\n  â†’ If >50% removed: Check if over-cleaning. Numbers/symbols may be important.\")\n",
    "print(f\"  â†’ If <10% removed: Check if under-cleaning. Noise may still be present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b73f5",
   "metadata": {},
   "source": [
    "# Implementing Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "909bfb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: typer>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "This DET\n",
      "app NOUN\n",
      "is AUX\n",
      "worst ADJ\n",
      "! PUNCT\n",
      "! PUNCT\n",
      "! PUNCT\n",
      "\n",
      "Sentences: [This app is worst!!!]\n",
      "Tokens: ['This', 'app', 'is', 'worst', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "import spacy\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Download the English model\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This app is worst!!!\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)  # Word + part-of-speech\n",
    "\n",
    "print(\"\\nSentences:\", list(doc.sents))\n",
    "print(\"Tokens:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f50eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ·ï¸  POS Tagging with Custom NER (Brand Names as ORG)\n",
      "================================================================================\n",
      "\n",
      "Token Analysis Table (Sample from first review):\n",
      "Token           Lemma        UPOS     Entity Label\n",
      "-------------------------------------------------------------------------------------\n",
      "Spotify         spotify      VERB     ORG         \n",
      "Is              be           AUX      AUX         \n",
      "very            very         ADV      ADV         \n",
      "good            good         ADJ      ADJ         \n",
      ",               ,            PUNCT    PUNCT       \n",
      "got             get          VERB     VERB        \n",
      "many            many         ADJ      ADJ         \n",
      "songs           song         NOUN     NOUN        \n",
      "And             and          CCONJ    CCONJ       \n",
      "suggests        suggest      VERB     VERB        \n",
      "New             new          ADJ      ADJ         \n",
      "ones            one          NOUN     NOUN        \n",
      "But             but          CCONJ    CCONJ       \n",
      "i               I            PRON     PRON        \n",
      "have            have         VERB     VERB        \n",
      "To              to           PART     PART        \n",
      "say             say          VERB     VERB        \n",
      "The             the          DET      DET         \n",
      "APP             APP          PROPN    PROPN       \n",
      "Was             be           AUX      AUX         \n",
      "\n",
      "================================================================================\n",
      "ğŸ“ Sentence-Level Analysis (Filtering Named Entities)\n",
      "================================================================================\n",
      "\n",
      "Sentence 1: Spotify Is very good, got many songs And suggests New ones\n",
      "  ğŸ“Œ Nouns: ['songs', 'ones']\n",
      "  âš™ï¸  Verbs: ['got', 'suggests']\n",
      "  âœ¨ Adjectives: ['good', 'many', 'New']\n",
      "  ğŸ¢ Named Entities (FILTERED OUT): ['Spotify']\n",
      "\n",
      "Sentence 2: But i have To say The APP Was way good before than Now.\n",
      "  âš™ï¸  Verbs: ['have', 'say']\n",
      "  âœ¨ Adjectives: ['good']\n",
      "\n",
      "Sentence 3: Because there are lot of ads showing up.\n",
      "  ğŸ“Œ Nouns: ['lot', 'ads']\n",
      "  âš™ï¸  Verbs: ['are', 'showing']\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š POS Distribution & Sentiment Words (Custom NER)\n",
      "================================================================================\n",
      "\n",
      "POS Tag Distribution:\n",
      "  NOUN    :  334 ( 13.9%)\n",
      "  VERB    :  304 ( 12.6%)\n",
      "  PROPN   :  234 (  9.7%)\n",
      "  PRON    :  227 (  9.4%)\n",
      "  ADJ     :  194 (  8.1%)\n",
      "  PUNCT   :  192 (  8.0%)\n",
      "  AUX     :  189 (  7.9%)\n",
      "  ADV     :  166 (  6.9%)\n",
      "  DET     :  161 (  6.7%)\n",
      "  ADP     :  159 (  6.6%)\n",
      "  PART    :  106 (  4.4%)\n",
      "  CCONJ   :   56 (  2.3%)\n",
      "  SCONJ   :   38 (  1.6%)\n",
      "  NUM     :   26 (  1.1%)\n",
      "  INTJ    :   13 (  0.5%)\n",
      "  X       :    3 (  0.1%)\n",
      "  SYM     :    2 (  0.1%)\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ Top Sentiment-Bearing Words (Spotify âœ“ Filtered)\n",
      "================================================================================\n",
      "\n",
      "Top 20 Sentiment Words by Frequency:\n",
      "   1. update          ( 34 occurrences)\n",
      "   2. bad             ( 21 occurrences)\n",
      "   3. feature         ( 18 occurrences)\n",
      "   4. basic           ( 17 occurrences)\n",
      "   5. song            ( 15 occurrences)\n",
      "   6. good            ( 14 occurrences)\n",
      "   7. have            ( 13 occurrences)\n",
      "   8. listen          ( 13 occurrences)\n",
      "   9. thing           ( 12 occurrences)\n",
      "  10. make            ( 12 occurrences)\n",
      "  11. remove          ( 12 occurrences)\n",
      "  12. play            ( 10 occurrences)\n",
      "  13. user            (  9 occurrences)\n",
      "  14. change          (  8 occurrences)\n",
      "  15. app             (  8 occurrences)\n",
      "  16. useless         (  8 occurrences)\n",
      "  17. use             (  8 occurrences)\n",
      "  18. get             (  7 occurrences)\n",
      "  19. day             (  7 occurrences)\n",
      "  20. say             (  6 occurrences)\n",
      "\n",
      "================================================================================\n",
      "âœ… Success!\n",
      "================================================================================\n",
      "\n",
      "The custom entity ruler now:\n",
      "  âœ“ Forces \"Spotify\" â†’ ORG entity (not VERB!)\n",
      "  âœ“ Filters it out from sentiment word counts\n",
      "  âœ“ Handles case variations (spotify, Spotify)\n",
      "  âœ“ Token display shows entity label when applicable\n",
      "  \n",
      "You can add more brands to the `patterns` list as needed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# â”€â”€ Add custom entity ruler for brand names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "# Define brand names and products to always mark as ORG\n",
    "patterns = [\n",
    "    {\"label\": \"ORG\", \"pattern\": \"Spotify\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": \"spotify\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": \"Apple Music\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": \"Google Play\"},\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# â”€â”€ Build capitalization map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ·ï¸  POS Tagging with Custom NER (Brand Names as ORG)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "capitalization_map = {}\n",
    "for raw_text in df['Content'].head(500):\n",
    "    for word in raw_text.split():\n",
    "        clean_word = word.rstrip(',.!?;:')\n",
    "        lower_word = clean_word.lower()\n",
    "        if clean_word and clean_word[0].isupper() and len(clean_word) > 1:\n",
    "            if lower_word not in capitalization_map:\n",
    "                capitalization_map[lower_word] = clean_word\n",
    "\n",
    "def restore_capitalization(text: str) -> str:\n",
    "    \"\"\"Restore capitalization before NER processing.\"\"\"\n",
    "    words = text.split()\n",
    "    restored = []\n",
    "    for word in words:\n",
    "        clean_word = word.rstrip(',.!?;:')\n",
    "        punct = word[len(clean_word):]\n",
    "        if clean_word.lower() in capitalization_map:\n",
    "            restored.append(capitalization_map[clean_word.lower()] + punct)\n",
    "        else:\n",
    "            restored.append(word)\n",
    "    return ' '.join(restored)\n",
    "\n",
    "# â”€â”€ POS Tagging with Custom NER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nToken Analysis Table (Sample from first review):\") \n",
    "print(f\"{'Token':<15} {'Lemma':<12} {'UPOS':<8} {'Entity Label':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "sample_text = df['cleaned'].iloc[0]\n",
    "sample_text_restored = restore_capitalization(sample_text)\n",
    "doc = nlp(sample_text_restored)\n",
    "\n",
    "# Build entity mapping: token text â†’ entity label\n",
    "entity_map = {}\n",
    "for ent in doc.ents:\n",
    "    for token in ent:\n",
    "        entity_map[token.text.lower()] = ent.label_\n",
    "\n",
    "for token in list(doc)[:20]:\n",
    "    # Show entity label if it's a named entity, otherwise show POS\n",
    "    if token.text.lower() in entity_map:\n",
    "        label = entity_map[token.text.lower()]\n",
    "    else:\n",
    "        label = token.pos_\n",
    "    print(f\"{token.text:<15} {token.lemma_:<12} {token.pos_:<8} {label:<12}\")\n",
    "\n",
    "# â”€â”€ Sentence-Level Extraction with Custom NER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ Sentence-Level Analysis (Filtering Named Entities)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sent in enumerate(list(doc.sents)[:3], 1):\n",
    "    print(f\"\\nSentence {i}: {sent.text}\")\n",
    "    \n",
    "    # Use custom NER to identify brand names\n",
    "    named_ents_in_sent = {ent.text.lower() for ent in sent.ents}\n",
    "    \n",
    "    nouns = [t.text for t in sent if t.pos_ == 'NOUN' and t.text.lower() not in named_ents_in_sent]\n",
    "    verbs = [t.text for t in sent if t.pos_ == 'VERB' and t.text.lower() not in named_ents_in_sent]\n",
    "    adjs = [t.text for t in sent if t.pos_ == 'ADJ']\n",
    "    propns = [ent.text for ent in sent.ents]\n",
    "    \n",
    "    if nouns:\n",
    "        print(f\"  ğŸ“Œ Nouns: {nouns}\")\n",
    "    if verbs:\n",
    "        print(f\"  âš™ï¸  Verbs: {verbs}\")\n",
    "    if adjs:\n",
    "        print(f\"  âœ¨ Adjectives: {adjs}\")\n",
    "    if propns:\n",
    "        print(f\"  ğŸ¢ Named Entities (FILTERED OUT): {propns}\")\n",
    "\n",
    "# â”€â”€ POS Distribution with Custom NER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š POS Distribution & Sentiment Words (Custom NER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pos_counts = {}\n",
    "lemma_freq = {}\n",
    "\n",
    "for review in df['cleaned'].head(100):\n",
    "    review_restored = restore_capitalization(review)\n",
    "    doc = nlp(review_restored)\n",
    "    \n",
    "    # Get named entities (custom ruler takes priority)\n",
    "    named_ents = {ent.text.lower() for ent in doc.ents}\n",
    "    \n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] = pos_counts.get(token.pos_, 0) + 1\n",
    "        \n",
    "        # Skip named entities like \"spotify\"\n",
    "        if token.pos_ in ['ADJ', 'VERB', 'NOUN'] and token.text.lower() not in named_ents:\n",
    "            lemma_freq[token.lemma_] = lemma_freq.get(token.lemma_, 0) + 1\n",
    "\n",
    "# Display POS distribution\n",
    "print(\"\\nPOS Tag Distribution:\")\n",
    "for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = (count / sum(pos_counts.values())) * 100\n",
    "    print(f\"  {pos:<8}: {count:>4} ({pct:>5.1f}%)\")\n",
    "\n",
    "# â”€â”€ Top Sentiment Words â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¡ Top Sentiment-Bearing Words (Spotify âœ“ Filtered)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sentiment_words = {\n",
    "    k: v for k, v in sorted(lemma_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    if len(k) > 2\n",
    "}\n",
    "\n",
    "print(\"\\nTop 20 Sentiment Words by Frequency:\")\n",
    "for i, (word, count) in enumerate(list(sentiment_words.items())[:20], 1):\n",
    "    print(f\"  {i:2d}. {word:<15} ({count:>3} occurrences)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Success!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "The custom entity ruler now:\n",
    "  âœ“ Forces \"Spotify\" â†’ ORG entity (not VERB!)\n",
    "  âœ“ Filters it out from sentiment word counts\n",
    "  âœ“ Handles case variations (spotify, Spotify)\n",
    "  âœ“ Token display shows entity label when applicable\n",
    "  \n",
    "You can add more brands to the `patterns` list as needed!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
